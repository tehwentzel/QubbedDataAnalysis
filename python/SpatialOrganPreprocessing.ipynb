{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatically reload stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the scripts I have to import the data,\n",
    "#save it as a intermediate json (patient_organ_data)\n",
    "#then denoise that and save it as a second json (patient_organ_data_denoise)\n",
    "#final version includes inputed values and the original mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simplejson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23566/642509505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSpatialPreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFormatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andrew/DATA1/git_repos/QubbedDataAnalysis/python/Utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msimplejson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simplejson'"
     ]
    }
   ],
   "source": [
    "import Utils\n",
    "import matplotlib.pyplot as plt\n",
    "from SpatialPreprocessing import *\n",
    "import pprint\n",
    "import Formatting\n",
    "from Constants import Const\n",
    "import copy\n",
    "import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_spatial_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23566/1722836653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read in the organ info and save it to a default format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspatial_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_spatial_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCamprtOrganData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m##for pid,files in spatial_files.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_spatial_files' is not defined"
     ]
    }
   ],
   "source": [
    "#read in the organ info and save it to a default format\n",
    "\n",
    "spatial_files = load_spatial_files()\n",
    "od = CamprtOrganData()\n",
    "##for pid,files in spatial_files.items():\n",
    "##    test1 = od.read_spatial_file(files['doses'])\n",
    "##    test2 = od.read_spatial_file(files['distances'])\n",
    "##     print(pid, test1.columns, test2[OrganData.roi_cols].values.ravel().tolist())\n",
    "pdict = od.process_cohort_spatial_dict(spatial_files)\n",
    "Utils.np_dict_to_json(pdict,Const.processed_organ_json)\n",
    "\n",
    "del pdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "31 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "31 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "31 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "31 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "31 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "31 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "31 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "37 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "37 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "37 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "37 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "37 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "37 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "37 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "37 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "105 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "105 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "105 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "105 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "105 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "105 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "105 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "105 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "112 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "112 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "112 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "112 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "121 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "121 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "121 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "121 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "133 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "133 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "133 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "133 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "165 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "165 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "165 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "165 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "171 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "171 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "171 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "171 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "184 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "184 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "184 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "184 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "184 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "184 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "220 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "220 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "220 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "220 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "232 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "232 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "232 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "232 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "236 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "236 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "236 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "236 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "260 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "260 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "260 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "260 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "260 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "260 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "260 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "260 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "262 Rt_Mastoid volume 0.0\n",
      "267 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "267 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "267 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "267 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "267 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "267 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "268 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "268 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "283 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "283 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10013 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10013 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10013 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10013 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10013 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10013 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10013 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10013 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10019 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10019 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10019 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10019 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10019 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10019 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10021 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10021 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "289 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "289 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "289 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "289 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "289 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "289 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "289 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "289 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10040 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10040 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10040 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10040 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10040 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10040 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10040 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10040 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10046 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10046 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10046 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10046 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10046 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10046 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10046 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10046 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10113 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10113 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10113 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10113 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10113 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10113 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10113 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10113 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10132 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10132 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10132 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10132 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10132 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10132 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10132 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10132 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10147 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10147 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10147 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10147 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10147 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10147 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10147 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10147 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5025 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "5025 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5025 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "5025 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10154 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10154 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10154 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10154 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10155 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10155 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10155 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10155 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10157 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10157 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10157 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10157 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10157 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10157 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5039 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5039 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5042 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "5042 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5042 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "5042 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5053 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "5053 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10176 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "10176 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "10176 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "10176 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10176 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "10176 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "10176 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "10176 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5059 Rt_Parotid_Gland mean_dose 0.0\n",
      "5059 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5059 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5059 Rt_Mastoid mean_dose 0.0\n",
      "2000 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "2000 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "2000 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "2000 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "2000 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "2000 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5077 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "5077 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "2009 Lt_thyroid_lobe volume 0.0\n",
      "2009 Lt_thyroid_lobe mean_dose 0.0\n",
      "2009 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "2009 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "2009 Rt_thyroid_lobe volume 0.0\n",
      "2009 Rt_thyroid_lobe mean_dose 0.0\n",
      "2009 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "2009 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "2009 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "2009 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "2009 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "2009 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5078 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "5078 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "5078 Lt_Posterior_Seg_Eyeball volume 0.0\n",
      "5078 Lt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "5078 Rt_Posterior_Seg_Eyeball volume 0.0\n",
      "5078 Rt_Posterior_Seg_Eyeball mean_dose 0.0\n",
      "5078 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "5078 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "2016 Lt_Anterior_Seg_Eyeball volume 0.0\n",
      "2016 Lt_Anterior_Seg_Eyeball mean_dose 0.0\n",
      "2016 Rt_Anterior_Seg_Eyeball volume 0.0\n",
      "2016 Rt_Anterior_Seg_Eyeball mean_dose 0.0\n"
     ]
    }
   ],
   "source": [
    "#this will be the format of the future data that is assumed for other functions\n",
    "#before transforming them into usable arrays\n",
    "#currently missing values are Nan.  Organs preserves the order in other arrays\n",
    "pdata = load_pdict()\n",
    "# pdata\n",
    "for pid,pentry in pdata['patients'].items():\n",
    "    for o,oentry in pentry.items():\n",
    "        for k,v in oentry.items():\n",
    "            if (not Utils.iterable(v) and v <= 0):\n",
    "                print(pid,o,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distances': [('Spinal_Cord', 1.81),\n",
       "  ('Lt_Posterior_Seg_Eyeball', 1.3575),\n",
       "  ('Rt_Brachial_Plexus', 0.905),\n",
       "  ('Thyroid_cartilage', 0.905),\n",
       "  ('Lt_Parotid_Gland', 0.905),\n",
       "  ('Rt_Ant_Digastric_M', 0.905),\n",
       "  ('Rt_Parotid_Gland', 0.4525),\n",
       "  ('Rt_Lateral_Pterygoid_M', 0.4525),\n",
       "  ('Lt_Mastoid', 0.4525),\n",
       "  ('Hyoid_bone', 0.4525),\n",
       "  ('Rt_Posterior_Seg_Eyeball', 0.4525),\n",
       "  ('Lt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Rt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Glottic_Area', 0.4525)],\n",
       " 'volume': [('Spinal_Cord', 1.81),\n",
       "  ('Lt_Posterior_Seg_Eyeball', 1.3575),\n",
       "  ('Esophagus', 0.905),\n",
       "  ('Rt_Brachial_Plexus', 0.905),\n",
       "  ('Thyroid_cartilage', 0.905),\n",
       "  ('Lt_Parotid_Gland', 0.905),\n",
       "  ('Rt_Ant_Digastric_M', 0.905),\n",
       "  ('Cricoid_cartilage', 0.4525),\n",
       "  ('Rt_Parotid_Gland', 0.4525),\n",
       "  ('Rt_Lateral_Pterygoid_M', 0.4525),\n",
       "  ('Lt_Mastoid', 0.4525),\n",
       "  ('Hyoid_bone', 0.4525),\n",
       "  ('Rt_Posterior_Seg_Eyeball', 0.4525),\n",
       "  ('Lt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Rt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Glottic_Area', 0.4525)],\n",
       " 'centroids': [('Spinal_Cord', 1.81),\n",
       "  ('Lt_Posterior_Seg_Eyeball', 1.3575),\n",
       "  ('Rt_Brachial_Plexus', 0.905),\n",
       "  ('Thyroid_cartilage', 0.905),\n",
       "  ('Lt_Parotid_Gland', 0.905),\n",
       "  ('Rt_Ant_Digastric_M', 0.905),\n",
       "  ('Rt_Parotid_Gland', 0.4525),\n",
       "  ('Rt_Lateral_Pterygoid_M', 0.4525),\n",
       "  ('Lt_Mastoid', 0.4525),\n",
       "  ('Hyoid_bone', 0.4525),\n",
       "  ('Rt_Posterior_Seg_Eyeball', 0.4525),\n",
       "  ('Lt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Rt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Glottic_Area', 0.4525)],\n",
       " 'mean_dose': [('Spinal_Cord', 1.81),\n",
       "  ('Lt_Posterior_Seg_Eyeball', 1.3575),\n",
       "  ('Rt_Brachial_Plexus', 0.905),\n",
       "  ('Thyroid_cartilage', 0.905),\n",
       "  ('Lt_Parotid_Gland', 0.905),\n",
       "  ('Rt_Ant_Digastric_M', 0.905),\n",
       "  ('Esophagus', 0.4525),\n",
       "  ('Cricoid_cartilage', 0.4525),\n",
       "  ('Rt_Parotid_Gland', 0.4525),\n",
       "  ('Rt_Lateral_Pterygoid_M', 0.4525),\n",
       "  ('Lt_Mastoid', 0.4525),\n",
       "  ('Hyoid_bone', 0.4525),\n",
       "  ('Rt_Posterior_Seg_Eyeball', 0.4525),\n",
       "  ('Lt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Rt_Anterior_Seg_Eyeball', 0.4525),\n",
       "  ('Glottic_Area', 0.4525)]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this should check that the values make sense\n",
    "#currently it implies there is an issue but I don't know what\n",
    "def get_num_nans_per_organ(sdata):\n",
    "    #should return a list of the % of patients missing data for each organ\n",
    "    #for each entry type\n",
    "    olist = sdata['organs']\n",
    "    oarlist = olist + ['gtv']\n",
    "    allnan = {}\n",
    "    for key in ['distances','volume','centroids','mean_dose']:\n",
    "        nancount = {o: 0 for o in oarlist}\n",
    "        arr = Formatting.merged_spatial_array(sdata, key)\n",
    "        for i,aa in enumerate(arr):\n",
    "            #find missing organs for individual\n",
    "            all_nan = np.argwhere(np.isnan(aa).all(axis=1))\n",
    "            nanset = set([])\n",
    "            if len(all_nan) < 1:\n",
    "                continue\n",
    "            for arg in all_nan[0]:\n",
    "                nanorgan = oarlist[arg]\n",
    "                nancount[nanorgan] = nancount[nanorgan] + 1\n",
    "        nancount = [(k,np.round(100*nancount[k]/arr.shape[0],4)) for k,v in nancount.items() if v > 0]\n",
    "        nancount = sorted(nancount, key = lambda x: -x[1])\n",
    "        \n",
    "        allnan[key] = nancount\n",
    "    return allnan\n",
    "get_num_nans_per_organ(pdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training stopped on epoch 2870\n",
      "OrganAutoEncoder(\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=2397, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=20, bias=True)\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=20, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=200, out_features=2397, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "tensor(0.1228, grad_fn=<MeanBackward0>)\n",
      "distances\n",
      "260.0000013049077 -24.640625 260.0 -24.640625\n",
      "260.0 -24.640625 \n",
      "\n",
      "mean_dose\n",
      "86.25 -1.7750456660081682e-09 86.25 0.0\n",
      "86.25 0.0 \n",
      "\n",
      "centroids\n",
      "658.3561 5.9474 658.3561 0.0\n",
      "658.3561 5.9474 \n",
      "\n",
      "volume\n",
      "891.0 0.0 891.0 0.0\n",
      "891.0 0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('distances', (221, 47, 46)),\n",
       " ('distances_missing', (221, 47, 46)),\n",
       " ('mean_dose', (221, 47, 1)),\n",
       " ('mean_dose_missing', (221, 47, 1)),\n",
       " ('centroids', (221, 47, 3)),\n",
       " ('centroids_missing', (221, 47, 3)),\n",
       " ('volume', (221, 47, 1)),\n",
       " ('volume_missing', (221, 47, 1))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converts the json to a dictionary of arrays with all missing values imputed\n",
    "#denoise alpha is the coefficient of how much of the densising is done \n",
    "#using the autoencoder used to input missing values [0 = none, 1 = all]\n",
    "di = Formatting.DataInputer(denoise_alpha = 0)\n",
    "ddict = di.get_formatted_arrays(pdata,retrain=True)\n",
    "[(k,v.shape) for k,v in ddict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_mean_reconstruction_error (%)</th>\n",
       "      <th>mean_denoised</th>\n",
       "      <th>mean_original</th>\n",
       "      <th>num_nan_denoised</th>\n",
       "      <th>num_nan_original</th>\n",
       "      <th>num_negative_denoised</th>\n",
       "      <th>num_negative_original</th>\n",
       "      <th>shape_denoised</th>\n",
       "      <th>shape_original</th>\n",
       "      <th>std_denoised</th>\n",
       "      <th>std_original</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>distances</th>\n",
       "      <td>7.903354</td>\n",
       "      <td>52.77325</td>\n",
       "      <td>52.61590</td>\n",
       "      <td>0</td>\n",
       "      <td>5732</td>\n",
       "      <td>17074</td>\n",
       "      <td>18250</td>\n",
       "      <td>(221, 47, 46)</td>\n",
       "      <td>(221, 47, 46)</td>\n",
       "      <td>41.22015</td>\n",
       "      <td>41.38671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_dose</th>\n",
       "      <td>66.938509</td>\n",
       "      <td>40.00218</td>\n",
       "      <td>39.29906</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>(221, 47, 1)</td>\n",
       "      <td>(221, 47, 1)</td>\n",
       "      <td>20.44255</td>\n",
       "      <td>21.81310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centroids</th>\n",
       "      <td>6.605791</td>\n",
       "      <td>171.54075</td>\n",
       "      <td>171.91481</td>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(221, 47, 3)</td>\n",
       "      <td>(221, 47, 3)</td>\n",
       "      <td>82.48657</td>\n",
       "      <td>82.80502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>21.497179</td>\n",
       "      <td>16.30311</td>\n",
       "      <td>16.57408</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(221, 47, 1)</td>\n",
       "      <td>(221, 47, 1)</td>\n",
       "      <td>25.35475</td>\n",
       "      <td>26.16836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           _mean_reconstruction_error (%)  mean_denoised  mean_original  \\\n",
       "key                                                                       \n",
       "distances                        7.903354       52.77325       52.61590   \n",
       "mean_dose                       66.938509       40.00218       39.29906   \n",
       "centroids                        6.605791      171.54075      171.91481   \n",
       "volume                          21.497179       16.30311       16.57408   \n",
       "\n",
       "           num_nan_denoised  num_nan_original  num_negative_denoised  \\\n",
       "key                                                                    \n",
       "distances                 0              5732                  17074   \n",
       "mean_dose                 0               117                      4   \n",
       "centroids                 0               204                      0   \n",
       "volume                    0               164                      0   \n",
       "\n",
       "           num_negative_original shape_denoised shape_original  std_denoised  \\\n",
       "key                                                                            \n",
       "distances                  18250  (221, 47, 46)  (221, 47, 46)      41.22015   \n",
       "mean_dose                      0   (221, 47, 1)   (221, 47, 1)      20.44255   \n",
       "centroids                      0   (221, 47, 3)   (221, 47, 3)      82.48657   \n",
       "volume                         0   (221, 47, 1)   (221, 47, 1)      25.35475   \n",
       "\n",
       "           std_original  \n",
       "key                      \n",
       "distances      41.38671  \n",
       "mean_dose      21.81310  \n",
       "centroids      82.80502  \n",
       "volume         26.16836  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the error on the neural net for inputing/denoising\n",
    "#only considers non-missing values\n",
    "#value are before performing clipping on the inputed values\n",
    "di.error_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.np_dict_to_json(ddict,Const.denoised_organ_array_dict,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GTVp',\n",
       "  {'volume': 10.8,\n",
       "   'mean_dose': 71.75,\n",
       "   'centroids': [279.6041, 198.22, 54.5221],\n",
       "   'distances': [83.0,\n",
       "    27.96875,\n",
       "    41.625,\n",
       "    82.5,\n",
       "    59.40625,\n",
       "    73.375,\n",
       "    72.5,\n",
       "    53.40625,\n",
       "    35.59375,\n",
       "    20.46875,\n",
       "    29.28125,\n",
       "    26.65625,\n",
       "    27.71875,\n",
       "    55.0625,\n",
       "    72.1875,\n",
       "    53.78125,\n",
       "    34.5625,\n",
       "    44.59375,\n",
       "    57.3125,\n",
       "    18.390625,\n",
       "    36.75,\n",
       "    12.84375,\n",
       "    -3.97265625,\n",
       "    -1.953125,\n",
       "    12.15625,\n",
       "    19.34375,\n",
       "    11.59375,\n",
       "    -4.98046875,\n",
       "    35.9375,\n",
       "    10.046875,\n",
       "    -4.02734375,\n",
       "    1.953125,\n",
       "    -3.3203125,\n",
       "    28.078125,\n",
       "    2.5,\n",
       "    6.1171875,\n",
       "    -3.90625,\n",
       "    6.90625,\n",
       "    23.9375,\n",
       "    56.6875,\n",
       "    70.9375,\n",
       "    73.0,\n",
       "    83.0,\n",
       "    58.09375,\n",
       "    58.625,\n",
       "    49.875]}),\n",
       " ('GTVn',\n",
       "  {'volume': 4.6,\n",
       "   'mean_dose': 72.28,\n",
       "   'centroids': [293.0174, 212.0342, 61.6571],\n",
       "   'distances': [52.1875,\n",
       "    29.421875,\n",
       "    6.734375,\n",
       "    63.90625,\n",
       "    30.453125,\n",
       "    39.90625,\n",
       "    54.6875,\n",
       "    29.796875,\n",
       "    14.4921875,\n",
       "    14.9453125,\n",
       "    43.40625,\n",
       "    20.015625,\n",
       "    11.65625,\n",
       "    59.21875,\n",
       "    89.6875,\n",
       "    66.6875,\n",
       "    61.5,\n",
       "    72.5625,\n",
       "    78.6875,\n",
       "    -1.953125,\n",
       "    29.796875,\n",
       "    0.9765625,\n",
       "    -4.3671875,\n",
       "    10.1484375,\n",
       "    32.71875,\n",
       "    14.9453125,\n",
       "    9.34375,\n",
       "    6.8359375,\n",
       "    48.5,\n",
       "    7.875,\n",
       "    26.109375,\n",
       "    21.3125,\n",
       "    25.03125,\n",
       "    45.5,\n",
       "    19.03125,\n",
       "    23.84375,\n",
       "    14.6875,\n",
       "    10.8828125,\n",
       "    58.59375,\n",
       "    86.375,\n",
       "    105.5625,\n",
       "    103.3125,\n",
       "    117.75,\n",
       "    84.625,\n",
       "    89.1875,\n",
       "    41.84375]})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pdict_gtvs(p):\n",
    "    gtvlist = []\n",
    "    for pid,pentry in p['patients'].items():\n",
    "        plist = []\n",
    "        for o,oentry in pentry.items():\n",
    "            if 'GTV' in o:\n",
    "                plist.append((o,oentry))\n",
    "        gtvlist.append((pid, plist))\n",
    "    return gtvlist\n",
    "\n",
    "#turn it into another dictionary again\n",
    "def inputed_data_dict(pdata, insert_gtvs = True):\n",
    "    pids = Formatting.get_sorted_pids(pdata)\n",
    "    organs = pdata['organs'] + ['gtv_composite']\n",
    "    main_keys = [k for k in ddict.keys() if '_missing' not in k]\n",
    "    mask_keys =[k for k in ddict.keys() if k not in main_keys]\n",
    "    make_dict = lambda keys: {int(p): {o: {k: np.nan for k in keys} for o in organs} for p in pids}\n",
    "    inputed_data = make_dict(main_keys)\n",
    "    data_mask = make_dict(mask_keys)\n",
    "    for k,varray in ddict.items():\n",
    "        for pid, v_row in zip(pids,varray):\n",
    "            for organ, o_col in zip(organs, v_row):\n",
    "                val = o_col\n",
    "                if len(val) <= 1:\n",
    "                    val = val[0]\n",
    "                if k in mask_keys:\n",
    "                    data_mask[int(pid)][organ][k.replace('_missing','')] = val\n",
    "                else:\n",
    "                    inputed_data[int(pid)][organ][k] = val\n",
    "    return_dict = {'organs': organs, 'patient_ids': pids, \n",
    "            'patients': inputed_data, 'mask': data_mask}\n",
    "    if insert_gtvs:\n",
    "        gtvset = extract_pdict_gtvs(pdata)\n",
    "        for pid,gtvs in gtvset:\n",
    "            for k,v in gtvs:\n",
    "                return_dict['patients'][int(pid)][k] = v\n",
    "    return return_dict\n",
    "\n",
    "inputed_ddict = inputed_data_dict(pdata)\n",
    "[(k,v) for k,v in inputed_ddict['patients'][185].items() if 'GTV' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hopefully there will be no nulls, but idk\n",
    "Utils.np_dict_to_json(inputed_ddict, Const.denoised_organ_json,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is code when I was trying to create pointclouds that \n",
    "#preserved the spatial features \n",
    "#(Does not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon,MultiPolygon\n",
    "\n",
    "def slice_ddict(pdata, ddict, organs = None):\n",
    "    if organs is None:\n",
    "        return {k:v for k,v in ddict.items()}\n",
    "    olist = pdata['organs']\n",
    "    indx = [olist.index(o) for o in organs]\n",
    "    new_dict = {}\n",
    "    for k,v in ddict.items():\n",
    "        new_val = v[:,indx + [-1],:]\n",
    "        if 'distance' in k:\n",
    "            new_val = new_val[:,:,indx]\n",
    "        new_dict[k] = new_val\n",
    "    return new_dict\n",
    "\n",
    "def radius_from_volume(v, eps = .01):\n",
    "    if v < eps:\n",
    "        v = eps\n",
    "    return (v*3/(4))**(1/3)\n",
    "\n",
    "def getSphereSize(npoints):\n",
    "    slices = int(np.sqrt(npoints))\n",
    "    stops = int(np.ceil(npoints/slices))\n",
    "    return slices, stops\n",
    "\n",
    "def sphere3d(center, radius, npoints):\n",
    "    slices, stops = getSphereSize(npoints)\n",
    "    #will not necessarily b npoints because its weird\n",
    "    pcloud = np.empty((slices*stops, 3))\n",
    "    pcloud[:] = center\n",
    "    theta_rotation = 2*np.pi/stops\n",
    "    phi_rotation = np.pi/slices\n",
    "    phi = 0\n",
    "    count = 0\n",
    "    for i in range(slices):\n",
    "        theta = 0\n",
    "        phi_vec = np.array([np.sin(phi),np.sin(phi),np.cos(phi)])\n",
    "        for ii in range(stops):\n",
    "            theta_vec = np.array([np.cos(theta),np.sin(theta), 1])\n",
    "            vector = radius * (theta_vec * phi_vec)\n",
    "            pcloud[count] += vector\n",
    "            theta += theta_rotation\n",
    "            count += 1\n",
    "        phi += phi_rotation\n",
    "    return pcloud\n",
    "            \n",
    "\n",
    "def sphere2d(center, radius, npoints):\n",
    "    pcloud = np.empty((npoints, 2))\n",
    "    pcloud[:] = center\n",
    "    rotation = 2*np.pi/(npoints)\n",
    "    angle = 0\n",
    "    for idx in range(npoints):\n",
    "        vector = [np.cos(angle), np.sin(angle)]\n",
    "        pcloud[idx] += radius*np.array(vector)\n",
    "        angle += rotation\n",
    "    return pcloud\n",
    "\n",
    "def make_pointcloud(centroid, volume,npoints):\n",
    "    #shape (ndim,) = (3,) (1,) (norgans,)\n",
    "    radius = radius_from_volume(volume[0])\n",
    "    ndims = centroid.shape[0]\n",
    "    if ndims == 2:\n",
    "        return sphere2d(centroid,radius,npoints)\n",
    "    else:\n",
    "        return sphere3d(centroid,radius,npoints)\n",
    "    \n",
    "def make_organset_pointcloud(centroids, volume, distances,npoints):\n",
    "    #n_organs x n_dim shaped inputs\n",
    "    n_organs = centroids.shape[0]\n",
    "    pclouds = []\n",
    "    for oidx in range(n_organs):\n",
    "        try:\n",
    "            organ_pcloud = make_pointcloud(centroids[oidx],\n",
    "                                           volume[oidx],\n",
    "                                           npoints)\n",
    "            pclouds.append(organ_pcloud)\n",
    "        except Exception as e:\n",
    "            print('error making single pcloud',e)\n",
    "            \n",
    "    pclouds = np.stack(pclouds)\n",
    "    return pclouds\n",
    "\n",
    "def polygon_shape_error(pgon,vol,centroid,alpha = .1, beta = 1):\n",
    "    vol_err = np.abs((vol[0] - pgon.area))/(vol[0])\n",
    "    center = [pgon.centroid.x,pgon.centroid.y]\n",
    "    c_err = np.linalg.norm(center - centroid)\n",
    "    return alpha*vol_err + beta*c_err\n",
    "\n",
    "def pcloud_loss(pcloud,vols,centroids,dists,vweight=.01,cweight=.5,dweight=1):\n",
    "    #n_organs + 1 x ndim shaped arrays\n",
    "    #pcloud (x npoints x ndim), vol (x1), centroids (x ndim) dists(x norgans)\n",
    "    err = 0\n",
    "    norgans = dists.shape[1]\n",
    "    pgons = [Polygon(pc) for pc in pcloud]\n",
    "    for i,pgon in enumerate(pgons):\n",
    "        shape_err = polygon_shape_error(pgon,vols[i],centroids[i],vweight,cweight)\n",
    "        d_err = 0\n",
    "        for odist, pgon2 in zip(dists[i],pgons):\n",
    "            d_err += np.abs((pgon.distance(pgon2) - odist)/odist)\n",
    "        d_err *= dweight/(len(pgons) - 1)\n",
    "    return d_err + shape_err\n",
    "\n",
    "def get_starter_pointclouds(pdata, ddict,organs=None,ndim = 2,npoints = None):\n",
    "    #should in-theory generate an array of polygons for each organ\n",
    "    #based around centroid\n",
    "    d = slice_ddict(pdata,ddict, organs)\n",
    "    pointclouds = []\n",
    "    n_patients = ddict['centroids'].shape[0]\n",
    "    vols = d['volume']\n",
    "    centroids = d['centroids']\n",
    "    dists = d['distances']\n",
    "    #do pca to reduce from 3 to 2 dimensions\n",
    "    assert(ndim in [2,3])\n",
    "    \n",
    "    #double check I did this right later\n",
    "    centroids = centroids - centroids.mean(axis=1).mean(axis=0)\n",
    "    #so I should probabl do some unit analysis here\n",
    "    vols = vols.clip(min=.1)#/vols.max(axis=0)\n",
    "    dists = dists.clip(min=.1)\n",
    "    if ndim == 2:\n",
    "        #so this should maybe be somehting idk\n",
    "        centroids = centroids[:,:,0:2]\n",
    "#         cshape = centroids.shape\n",
    "#         c = centroids.reshape((-1,3))\n",
    "#         print('staring mds')\n",
    "#         c = manifold.Isomap().fit_transform(c)\n",
    "#         print('finished')\n",
    "#         centroids = c.reshape((cshape[0],cshape[1],ndim))\n",
    "    if npoints is None:\n",
    "        npoints = 10**(ndim-1)\n",
    "    for idx in range(n_patients):\n",
    "        try:\n",
    "            pcloud = make_organset_pointcloud(centroids[idx],vols[idx],dists[idx],npoints)\n",
    "            pointclouds.append(pcloud)\n",
    "        except Exception as e:\n",
    "            print('error making pointcloud')\n",
    "            print(e)\n",
    "    #will be n_patients, n_organs, n_points, n_dimensions\n",
    "    #so output[x] will be a multipolygon for a patient \n",
    "    #and output[x,y] will be a pointcloud for an organ\n",
    "    return np.stack(pointclouds)\n",
    "        \n",
    "#for some reason the masseter is alway way out there\n",
    "test_organs = [o for o in pdata['organs']]# if \"Masset\" not in o]\n",
    "pcs = get_starter_pointclouds(pdata,ddict, ndim = 2,organs=test_organs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([Polygon(p) for pc in pcs[0:1] for p in pc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs[0,0].var(axis=0)/ddict['volume'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from Pytorchtools import EarlyStopping\n",
    "from SpatialLosses import *\n",
    "class PointCloudOptimizer(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 norgans,\n",
    "                 ndim = 2,\n",
    "                 npoints = 10,\n",
    "                 hidden_dims = [1000]\n",
    "                ):\n",
    "        super(PointCloudOptimizer,self).__init__()\n",
    "        self.ndim = ndim\n",
    "        #I have to change the point size to work with spheres evenly\n",
    "        self.npoints = npoints\n",
    "        if ndim == 3: \n",
    "            i,ii = getSphereSize(npoints)\n",
    "            self.true_npoints = ii*i\n",
    "        else:\n",
    "            self.true_npoints = npoints\n",
    "        self.norgans = norgans\n",
    "        self.pcloud_size = int(ndim*npoints)\n",
    "        input_organ_size = self.pcloud_size + norgans + ndim + 1\n",
    "        layers = [ nn.Linear(norgans*input_organ_size,hidden_dims[0]) ]\n",
    "        for i in range(0,len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(hidden_dims[i],hidden_dims[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            \n",
    "        self.output_shape = (norgans,npoints,ndim)\n",
    "        layers.append(nn.Linear(hidden_dims[-1], norgans*npoints*ndim))\n",
    "        self.layers = nn.Sequential( *layers )\n",
    "        self.layers.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self,layer):\n",
    "        #make the transform weights zero to start\n",
    "        #so the output is always a valid polygon when it starts\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.normal_(layer.weight,mean=0.0,std=0.0)\n",
    "        \n",
    "    def make_pclouds(self,x):\n",
    "        #takes an entry from the centroid, volumes and distances of a patient\n",
    "        #converts from numpy to torch also\n",
    "        #c (centroids): norgans x ndim, \n",
    "        #v (volume): norgans x 1, \n",
    "        #d (distances) norgans x norgans\n",
    "        [cs,vs,ds] = [i.cpu().detach().numpy() for i in x]\n",
    "        pclouds = [make_organset_pointcloud(c,v,d,self.npoints) for c,v,d in zip(cs,vs,ds)]\n",
    "        return torch.tensor(pclouds).float()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x is a list [c,v,d] of np arrays\n",
    "        #c (centroids): len x norgans x ndim, \n",
    "        #v (volume): len x norgans x 1, \n",
    "        #d (distances) len x norgans x norgans\n",
    "        \n",
    "        #makes torch friendly\n",
    "        #shape len x norgans x true_npoints x ndim\n",
    "        pointclouds = self.make_pclouds(x)\n",
    "        pc_shape = pointclouds.shape\n",
    "        #reshape so it can concatenate with other inputs\n",
    "        pointclouds = pointclouds.reshape((pc_shape[0],pc_shape[1],-1))\n",
    "        #now is len x norgans x norgans + ndim + (npoints*ndim)\n",
    "        out = torch.flatten(torch.cat([pointclouds] + x,dim=2),start_dim=1)\n",
    "        out = self.layers(out).reshape(pc_shape)\n",
    "        pointclouds = pointclouds.reshape(pc_shape)\n",
    "        return out + pointclouds, pointclouds     \n",
    "\n",
    "def format_pcloud_inputs(ddict):\n",
    "    oidx = [i for i,o in enumerate(pdata['organs']) if \"Masset\" not in o]\n",
    "    v = ddict['volume'][:pcount,oidx].clip(min=.1)\n",
    "    d = ddict['distances'][:pcount,oidx]\n",
    "    d = d[:,:,oidx].clip(min=.01)\n",
    "    #I think volume is in cm3 and distance is mm ?\n",
    "    d = 10*d\n",
    "    c = ddict['centroids'][:pcount,oidx,0:ndim]\n",
    "    c = c - c.mean(axis=0)\n",
    "    [c, v, d] = [torch.tensor(i).float() for i in [c,v,d]]\n",
    "    return c, v, d\n",
    "\n",
    "def train_pcloud_optimizer(ddict, \n",
    "                        ndim = 2,\n",
    "                        npoints = 10,\n",
    "                        model_path = Const.pytorch_model_dir + 'pcloud.pt',\n",
    "                        epochs = 1000,\n",
    "                        lr = .001,\n",
    "                        centroid_weight = .5,\n",
    "                        shape_weight = .2,\n",
    "                        distance_weight = 1\n",
    "                        ):\n",
    "    pcount = 20\n",
    "    c,v,d = format_pcloud_inputs(ddict)\n",
    "\n",
    "    popt = PointCloudOptimizer(c.shape[1],ndim,npoints = npoints)\n",
    "    early_stopping = EarlyStopping(patience = 10, path = model_path)\n",
    "    optimizer = torch.optim.SGD(popt.parameters(),lr =lr)\n",
    "    loss_hist = []\n",
    "#     criterion = PointCloudLoss(1,.5)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        ypred, ydefault = popt([c,v,d])\n",
    "        loss = pcloud_loss(ypred,ydefault,\n",
    "                           c,v,d,\n",
    "                           shape_weight,centroid_weight,distance_weight)\n",
    "        loss.backward()\n",
    "        loss_hist.append(loss.item())\n",
    "        print('epoch',epoch,loss.item())\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        early_stopping(loss.item(),popt)\n",
    "        if early_stopping.early_stop or torch.isnan(loss):\n",
    "            print('early stop on epoch',epoch)\n",
    "            break\n",
    "    popt.load_state_dict(torch.load(model_path))\n",
    "    plt.plot(loss_hist)\n",
    "    return popt.eval(), loss_hist\n",
    "\n",
    "popt, lh = train_pcloud_optimizer(ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint\n",
    "pcount = 10\n",
    "ndim = 2\n",
    "c,v,d = format_pcloud_inputs(ddict)\n",
    "ypred,ydefault = popt([c,v,d])\n",
    "clouds = ypred.cpu().detach().numpy()\n",
    "MultiPolygon([Polygon(p) for pc in clouds[0:1] for p in pc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the part below is generic notebook analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many organs are missing in the old dataset\n",
    "sf = load_spatial_files()\n",
    "organ_counts = {}\n",
    "total_patients = 0\n",
    "for pid, files in sf.items():\n",
    "    dosefile = files['doses']\n",
    "    distancefile = files['distances']\n",
    "    def read_replace(file):\n",
    "        df = pd.read_csv(file)\n",
    "        df.replace(to_replace = r'_*GTV.*N', value = '_GTVn', regex = True, inplace = True)\n",
    "        return df\n",
    "    dosedf = read_replace(dosefile)\n",
    "    distdf = read_replace(distancefile)\n",
    "    dose_rois = set(dosedf['ROI'].dropna())\n",
    "    dist_rois = set(distdf['Reference ROI'].dropna()).union(set(distdf['Target ROI'].dropna()))\n",
    "    rois = dose_rois.intersection(dist_rois)\n",
    "    has_gtv = False\n",
    "    for r in rois:\n",
    "        if 'GTV' in r:\n",
    "            has_gtv = True\n",
    "    if not has_gtv:\n",
    "        continue\n",
    "    else:\n",
    "        total_patients += 1\n",
    "    for roi in rois:\n",
    "        if 'GTV' in roi:\n",
    "            continue\n",
    "        currcount = organ_counts.get(roi, 0)\n",
    "        organ_counts[roi] = currcount + 1\n",
    "def tempsort(val):\n",
    "    key = val[0]\n",
    "    if key in Const.camprt_organs:\n",
    "        idx = Const.camprt_organs.index(key)\n",
    "    else:\n",
    "        idx = 0\n",
    "    return (.5*val[1]/total_patients) + (2*idx/len(Const.camprt_organs))\n",
    "values = sorted([(k,v) for k,v in organ_counts.items()], key = tempsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "axis = [i for i,v in enumerate(values)]\n",
    "widths = [x[1] for x in values]\n",
    "labels = [x[0] for x in values]\n",
    "colors = []\n",
    "for label, val in values:\n",
    "    if label in Const.camprt_organs:\n",
    "        if val >= total_patients:\n",
    "            c ='blue'\n",
    "        else:\n",
    "            c = 'cyan'\n",
    "    else:\n",
    "        c = 'red'\n",
    "    colors.append(c)\n",
    "plt.rcParams['figure.figsize'] = [10,30]\n",
    "fix, ax = plt.subplots()\n",
    "ax.barh(axis,widths,\n",
    "         tick_label=labels,\n",
    "        color = colors,\n",
    "        log = False)\n",
    "ax.plot([total_patients,total_patients],[0,len(axis)],\"k--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
